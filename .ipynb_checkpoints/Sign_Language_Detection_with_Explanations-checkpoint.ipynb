{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6c251",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "# Explanation: import pandas as pd\n",
    "import numpy as np\n",
    "# Explanation: import numpy as np\n",
    "import torch\n",
    "# Explanation: import torch\n",
    "import torch.nn as nn\n",
    "# Explanation: import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Explanation: import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Explanation: from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Explanation: from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Explanation: from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "# Explanation: from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "# Explanation: import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Load the dataset\n",
    "train_df = pd.read_csv('sign_mnist_train.csv')\n",
    "test_df = pd.read_csv('sign_mnist_test.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da82368",
   "metadata": {},
   "source": [
    "train_df = pd.read_csv('sign_mnist_train.csv')\n",
    "# Explanation: train_df = pd.read_csv('sign_mnist_train.csv')\n",
    "test_df = pd.read_csv('sign_mnist_test.csv')\n",
    "# Explanation: test_df = pd.read_csv('sign_mnist_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d4c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: Check the dataset\n",
    "print(\"Training Data Shape: \", train_df.shape)\n",
    "print(\"Testing Data Shape: \", test_df.shape)\n",
    "train_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b9ef4",
   "metadata": {},
   "source": [
    "print(\"Training Data Shape: \", train_df.shape)\n",
    "# Explanation: print(\"Training Data Shape: \", train_df.shape)\n",
    "print(\"Testing Data Shape: \", test_df.shape)\n",
    "# Explanation: print(\"Testing Data Shape: \", test_df.shape)\n",
    "train_df.head()\n",
    "# Explanation: train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035300d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Separate features and labels\n",
    "X_train = train_df.iloc[:, 1:].values\n",
    "y_train = train_df.iloc[:, 0].values\n",
    "X_test = test_df.iloc[:, 1:].values\n",
    "y_test = test_df.iloc[:, 0].values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1659d",
   "metadata": {},
   "source": [
    "X_train = train_df.iloc[:, 1:].values\n",
    "# Explanation: X_train = train_df.iloc[:, 1:].values\n",
    "y_train = train_df.iloc[:, 0].values\n",
    "# Explanation: y_train = train_df.iloc[:, 0].values\n",
    "X_test = test_df.iloc[:, 1:].values\n",
    "# Explanation: X_test = test_df.iloc[:, 1:].values\n",
    "y_test = test_df.iloc[:, 0].values\n",
    "# Explanation: y_test = test_df.iloc[:, 0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 5: Adjust labels to be in range 0-23 (since we have 24 classes)\n",
    "y_train = np.where(y_train > 8, y_train - 1, y_train)\n",
    "y_test = np.where(y_test > 8, y_test - 1, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a010ee3",
   "metadata": {},
   "source": [
    "y_train = np.where(y_train > 8, y_train - 1, y_train)\n",
    "# Explanation: y_train = np.where(y_train > 8, y_train - 1, y_train)\n",
    "y_test = np.where(y_test > 8, y_test - 1, y_test)\n",
    "# Explanation: y_test = np.where(y_test > 8, y_test - 1, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: Normalize the pixel values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83804853",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "# Explanation: scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Explanation: X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Explanation: X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 7: Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923a3a7",
   "metadata": {},
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# Explanation: X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# Explanation: y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# Explanation: X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "# Explanation: y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1686b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 8: Create DataLoader for training and testing\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc604cb2",
   "metadata": {},
   "source": [
    "batch_size = 64\n",
    "# Explanation: batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# Explanation: train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Explanation: test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Explanation: train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Explanation: test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 9: Plot some sample images from the dataset\n",
    "def plot_sample_images(X, y, n=10):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Label: {y[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_images(X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4fbb4",
   "metadata": {},
   "source": [
    "def plot_sample_images(X, y, n=10):\n",
    "# Explanation: def plot_sample_images(X, y, n=10):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "# Explanation:     plt.figure(figsize=(10, 10))\n",
    "    for i in range(n):\n",
    "# Explanation:     for i in range(n):\n",
    "        plt.subplot(1, n, i+1)\n",
    "# Explanation:         plt.subplot(1, n, i+1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "# Explanation:         plt.imshow(X[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Label: {y[i]}\")\n",
    "# Explanation:         plt.title(f\"Label: {y[i]}\")\n",
    "        plt.axis('off')\n",
    "# Explanation:         plt.axis('off')\n",
    "    plt.show()\n",
    "# Explanation:     plt.show()\n",
    "plot_sample_images(X_train, y_train)\n",
    "# Explanation: plot_sample_images(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d67cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 10: Define the ANN model\n",
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN_Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = ANN_Model()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d1869",
   "metadata": {},
   "source": [
    "class ANN_Model(nn.Module):\n",
    "# Explanation: class ANN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "# Explanation:     def __init__(self):\n",
    "        super(ANN_Model, self).__init__()\n",
    "# Explanation:         super(ANN_Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 512)\n",
    "# Explanation:         self.layer1 = nn.Linear(784, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "# Explanation:         self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "# Explanation:         self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 24)\n",
    "# Explanation:         self.layer4 = nn.Linear(128, 24)\n",
    "        self.relu = nn.ReLU()\n",
    "# Explanation:         self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "# Explanation:         self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, x):\n",
    "# Explanation:     def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "# Explanation:         x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "# Explanation:         x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "# Explanation:         x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "# Explanation:         x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "# Explanation:         x = self.relu(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "# Explanation:         x = self.layer4(x)\n",
    "        return x\n",
    "# Explanation:         return x\n",
    "model = ANN_Model()\n",
    "# Explanation: model = ANN_Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4143a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 11: Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297ea19",
   "metadata": {},
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Explanation: criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Explanation: optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 12: Training the model with validation\n",
    "epochs = 20\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train_part, X_valid_part, y_train_part, y_valid_part = train_test_split(X_train_tensor, y_train_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset_part = TensorDataset(X_train_part, y_train_part)\n",
    "valid_dataset_part = TensorDataset(X_valid_part, y_valid_part)\n",
    "\n",
    "train_loader_part = DataLoader(train_dataset_part, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_part = DataLoader(valid_dataset_part, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader_part:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader_part)\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader_part:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    valid_loss = running_loss / len(valid_loader_part)\n",
    "    valid_accuracy = correct / total\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Valid Loss: {valid_loss}, Valid Accuracy: {valid_accuracy}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252169c",
   "metadata": {},
   "source": [
    "epochs = 20\n",
    "# Explanation: epochs = 20\n",
    "train_losses = []\n",
    "# Explanation: train_losses = []\n",
    "valid_losses = []\n",
    "# Explanation: valid_losses = []\n",
    "train_accuracies = []\n",
    "# Explanation: train_accuracies = []\n",
    "valid_accuracies = []\n",
    "# Explanation: valid_accuracies = []\n",
    "X_train_part, X_valid_part, y_train_part, y_valid_part = train_test_split(X_train_tensor, y_train_tensor, test_size=0.2, random_state=42)\n",
    "# Explanation: X_train_part, X_valid_part, y_train_part, y_valid_part = train_test_split(X_train_tensor, y_train_tensor, test_size=0.2, random_state=42)\n",
    "train_dataset_part = TensorDataset(X_train_part, y_train_part)\n",
    "# Explanation: train_dataset_part = TensorDataset(X_train_part, y_train_part)\n",
    "valid_dataset_part = TensorDataset(X_valid_part, y_valid_part)\n",
    "# Explanation: valid_dataset_part = TensorDataset(X_valid_part, y_valid_part)\n",
    "train_loader_part = DataLoader(train_dataset_part, batch_size=batch_size, shuffle=True)\n",
    "# Explanation: train_loader_part = DataLoader(train_dataset_part, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_part = DataLoader(valid_dataset_part, batch_size=batch_size, shuffle=False)\n",
    "# Explanation: valid_loader_part = DataLoader(valid_dataset_part, batch_size=batch_size, shuffle=False)\n",
    "for epoch in range(epochs):\n",
    "# Explanation: for epoch in range(epochs):\n",
    "    model.train()\n",
    "# Explanation:     model.train()\n",
    "    running_loss = 0.0\n",
    "# Explanation:     running_loss = 0.0\n",
    "    correct = 0\n",
    "# Explanation:     correct = 0\n",
    "    total = 0\n",
    "# Explanation:     total = 0\n",
    "    for inputs, labels in train_loader_part:\n",
    "# Explanation:     for inputs, labels in train_loader_part:\n",
    "        optimizer.zero_grad()\n",
    "# Explanation:         optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "# Explanation:         outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "# Explanation:         loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "# Explanation:         loss.backward()\n",
    "        optimizer.step()\n",
    "# Explanation:         optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "# Explanation:         running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "# Explanation:         _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "# Explanation:         total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "# Explanation:         correct += (predicted == labels).sum().item()\n",
    "    train_loss = running_loss / len(train_loader_part)\n",
    "# Explanation:     train_loss = running_loss / len(train_loader_part)\n",
    "    train_accuracy = correct / total\n",
    "# Explanation:     train_accuracy = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "# Explanation:     train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "# Explanation:     train_accuracies.append(train_accuracy)\n",
    "    model.eval()\n",
    "# Explanation:     model.eval()\n",
    "    running_loss = 0.0\n",
    "# Explanation:     running_loss = 0.0\n",
    "    correct = 0\n",
    "# Explanation:     correct = 0\n",
    "    total = 0\n",
    "# Explanation:     total = 0\n",
    "    with torch.no_grad():\n",
    "# Explanation:     with torch.no_grad():\n",
    "        for inputs, labels in valid_loader_part:\n",
    "# Explanation:         for inputs, labels in valid_loader_part:\n",
    "            outputs = model(inputs)\n",
    "# Explanation:             outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "# Explanation:             loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "# Explanation:             running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "# Explanation:             _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "# Explanation:             total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "# Explanation:             correct += (predicted == labels).sum().item()\n",
    "    valid_loss = running_loss / len(valid_loader_part)\n",
    "# Explanation:     valid_loss = running_loss / len(valid_loader_part)\n",
    "    valid_accuracy = correct / total\n",
    "# Explanation:     valid_accuracy = correct / total\n",
    "    valid_losses.append(valid_loss)\n",
    "# Explanation:     valid_losses.append(valid_loss)\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "# Explanation:     valid_accuracies.append(valid_accuracy)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Valid Loss: {valid_loss}, Valid Accuracy: {valid_accuracy}\")\n",
    "# Explanation:     print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Valid Loss: {valid_loss}, Valid Accuracy: {valid_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec368af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 13: Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs+1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40e23e",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "# Explanation: plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
    "# Explanation: plt.plot(range(1, epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs+1), valid_losses, label='Validation Loss')\n",
    "# Explanation: plt.plot(range(1, epochs+1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "# Explanation: plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "# Explanation: plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# Explanation: plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "# Explanation: plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "# Explanation: plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff32935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 14: Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, epochs+1), valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555cd21",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "# Explanation: plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label='Training Accuracy')\n",
    "# Explanation: plt.plot(range(1, epochs+1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, epochs+1), valid_accuracies, label='Validation Accuracy')\n",
    "# Explanation: plt.plot(range(1, epochs+1), valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "# Explanation: plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "# Explanation: plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "# Explanation: plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "# Explanation: plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "# Explanation: plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 15: Evaluate the model on the test set\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "        y_true.extend(labels.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02e0ad",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "# Explanation: model.eval()\n",
    "y_pred = []\n",
    "# Explanation: y_pred = []\n",
    "y_true = []\n",
    "# Explanation: y_true = []\n",
    "with torch.no_grad():\n",
    "# Explanation: with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "# Explanation:     for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "# Explanation:         outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "# Explanation:         _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "# Explanation:         y_pred.extend(predicted.numpy())\n",
    "        y_true.extend(labels.numpy())\n",
    "# Explanation:         y_true.extend(labels.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827db028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 16: Calculate accuracy and F1-score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28008291",
   "metadata": {},
   "source": [
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "# Explanation: accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "# Explanation: f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# Explanation: print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "# Explanation: print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758795f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 17: Display the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(conf_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1846ca",
   "metadata": {},
   "source": [
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# Explanation: conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(conf_matrix)\n",
    "# Explanation: print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 18: Display the classification report\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(class_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be2a6d",
   "metadata": {},
   "source": [
    "class_report = classification_report(y_true, y_pred)\n",
    "# Explanation: class_report = classification_report(y_true, y_pred)\n",
    "print(class_report)\n",
    "# Explanation: print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3956a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
